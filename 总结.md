# 项目总结
1. 查看模型文档，不如直接查看模型代码！！！
2. huggingface中的transformers库中的LlamaRotaryEmbedding实现是GPT-NeoX style, 并且先在LlamaModel的forward中计算position_embeddings，再将其传入self_attention的forward中，在forward中分解position_embeddings为(cos, sin)，后配合投影后的query_states和key_states，应用apply_rotary_pos_emb。
3. 注意pytorch中的张量的连续性、shape和stride()，要熟悉view、reshape、transpose等方法，reshape和transpose不同，不要想当然的把reshape当成transpose来用，要用transpose交换维度，用reshape来分解或聚合维度
